from joblib import Parallel, delayed
from sklearn.model_selection import ParameterGrid
from .base import BaseOptimizer, evaluate_params
from .params import OptimizationParams
from .shared_state import SharedState

class GridSearchOptimizer(BaseOptimizer):
    def validate_params(self, params: OptimizationParams) -> None:
        if params.n_jobs is not None and params.n_jobs == 0:
            raise ValueError("n_jobsä¸èƒ½ä¸º0")

    def optimize(self, data, params: OptimizationParams, shared_state: SharedState):
        # éªŒè¯å‚æ•°
        self.validate_params(params)
        
        # ä»base_configè·å–å‚æ•°èŒƒå›´
        param_ranges = self.base_config.get('optimizer', {}).get('param_ranges', {})
        if not param_ranges:
            raise ValueError("æœªåœ¨é…ç½®ä¸­æ‰¾åˆ°å‚æ•°ä¼˜åŒ–èŒƒå›´é…ç½®(optimizer.param_ranges)")
        
        # ç”Ÿæˆå‚æ•°ç½‘æ ¼
        param_grid = list(ParameterGrid(param_ranges))
        print(f"\nğŸ“Š å¼€å§‹ç½‘æ ¼æœç´¢ï¼Œå‚æ•°ç»„åˆæ•°: {len(param_grid)}")
        
        # å¹¶è¡Œæ‰§è¡Œå›æµ‹ï¼Œä¼ é€’å…±äº«çŠ¶æ€ç®¡ç†å™¨å®ä¾‹
        results = Parallel(n_jobs=params.n_jobs, verbose=50)(
            delayed(evaluate_params)(
                param, 
                data, 
                self.base_config, 
                shared_state,  # ä¼ é€’å…±äº«çŠ¶æ€ç®¡ç†å™¨å®ä¾‹
                params.verbose
            )
            for param in param_grid
        )
        
        # è¿‡æ»¤æ— æ•ˆç»“æœå¹¶æ’åº
        valid_results = [r for r in results if r.get('metrics')]
        return sorted(valid_results, key=lambda x: x['metrics']['sharpe_ratio'], reverse=True)